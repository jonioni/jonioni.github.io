<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Backpropagation Through Time</title>
    <style>
        body {
            font-family: monospace;
            font-size: 14pt;
            font-weight: lighter;
            word-wrap: break-word;
            margin: auto;
            padding: 1rem;
            max-width: 48rem;
        }
        h1 {
            text-align: center;
        }
        div, img {
            display: block;
            width: 100%;
            margin: auto;
        }
        a {
            color: black;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        li {
            margin: 1rem;
        }
        code {
            color: salmon;
        }
        pre {
            white-space: pre-wrap;
            border: dashed lightgray;
            border-radius: 0.5rem;
        }
        table {
            width: 100%;
            border-collapse: collapse;
        }
        th, td {
            border-bottom: 1px solid lightgray;
        }
        q, blockquote {
            quotes: "❝" "❞" "❛" "❜";
        }
        blockquote:before {
            content: open-quote;
            float: left;
            color: lightgray;
            font-size: 3rem;
        }
        blockquote p {
            margin-left: 3rem;
        }
        blockquote:after {
            content: no-close-quote;
        }
        h1:before,
        h2:before,
        h3:before,
        h4:before,
        h5:before {
            color: lightgray;
            font-style: italic;
            font-size: smaller;
        }
        /* h1:before { content: '# '; } */
        /* h2:before { content: '## '; } */
        /* h3:before { content: '### '; } */
        /* h4:before { content: '#### '; } */
        /* h5:before { content: '##### '; } */        html {
    height: 100%;
}

body {
    height: 100%;
    display: flex;
    flex-direction: column;
}

#content {
    flex-grow: 1;
}

#preamble {
    width: 100%;
}

#postamble {
    width: 100%;
    text-align: right;
}        h1 { counter-reset: h2counter; }
h2 { counter-reset: h3counter; }
h3 { counter-reset: h4counter; }
h4 { counter-reset: h5counter; }
h5 { counter-reset: h6counter; }
h2:before {
  counter-increment: h2counter;
  content: counter(h2counter) " ";
}
h3:before {
  counter-increment: h3counter;
  content: counter(h2counter) "." counter(h3counter) " ";
}
h4:before {
  counter-increment: h4counter;
  content: counter(h2counter) "." counter(h3counter) "." counter(h4counter) " ";
}
h5:before {
  counter-increment: h5counter;
  content: counter(h2counter) "." counter(h3counter) "." counter(h4counter) "." counter(h5counter) " ";
}
h6:before {
  counter-increment: h6counter;
  content: counter(h2counter) "." counter(h3counter) "." counter(h4counter) "." counter(h5counter) "." counter(h6counter) " ";
}        .theorem {
    display:block;
    clear: both;
    font-style: italic;
}

.theorem:before {
    content:"Theorem. ";
    float: left;
    font-weight:bold;
    font-style: normal;
}

.lemma {
    display:block;
    clear: both;
    font-style: italic;
}

.lemma:before{
    content:"Lemma. ";
    float: left;
    font-weight:bold;
    font-style: normal;
}

.proof {
    display:block;
    clear: both;
    font-style: normal;
}

.proof p:last-child {
    display: inline;
}

.proof:before{
    content:'Proof. ';
    float: left;
    font-weight:bold;
    font-style: italic;
}

.proof:after{
    content:"◼";
    float:right;
    font-size: smaller;
}

.definition {
    display:block;
    clear: both;
    font-style: normal;
}

.definition:before {
    content: "Definition. ";
    float: left;
    font-weight: bold;
    font-style: italic;
}    </style>
</head>

<body>
    <div id="preamble"></div>
    <div id="content">
        <h1>Backpropagation Through Time (BPTT)</h1>
<p>"Just a fancy name for standard backpropagation on an unrolled RNN."</p>
<h2>Formula</h2>
<p>Suppose the network is defined as:
$$
\begin{aligned}
s_t 
    &= \tanh(Ux_t + Ws_{t-1}) \\
\hat{y}_t
    &= \text{softmax}(Vs_t)
\end{aligned}
$$
The error is given by:
$$
\begin{aligned}
\mathcal{L}(y, \hat{y})
    &= \sum_{t} \mathcal{L}_t(y_t, \hat{y_t}) = -\sum_{t} y_t \log \hat{y_t}
\end{aligned}
$$
We treat the full sequence(e.g. sentence) as one training example, so the total error is just the sum of the errors at each time step(e.g. word).</p>
<p>The gradient:
$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial W} 
    &= \sum_{t} \frac{\partial \mathcal{L}_t}{\partial W}
\end{aligned}
$$
For $V$, the gradient is independent from the states:
$$
\begin{aligned}
\frac{\partial \mathcal{L}_3}{\partial V}
    &= \frac{\partial \mathcal{L}_3}{\partial \hat{y_3}} \cdot \frac{\partial \hat{y_3}}{\partial V}
    = \frac{\partial \mathcal{L}_3}{\partial \hat{y_3}} \cdot \frac{\partial \hat{y_3}}{\partial z_3} \cdot \frac{\partial z_3}{\partial V} 
    = \frac{\partial \mathcal{L}_3}{\partial z_3} \cdot \frac{\partial z_3}{\partial V} 
    = (\hat{y_3} - y_3) \cdot s_3
\end{aligned}
$$
For $W$, the situation is more complicated:
$$
\begin{aligned}
\frac{\partial \mathcal{L}_3}{\partial W}
    &= \frac{\partial \mathcal{L}_3}{\partial \hat{y_3}} \cdot \frac{\partial \hat{y_3}}{\partial s_3} \cdot \frac{\partial s_3}{\partial W} \\
    &= \frac{\partial \mathcal{L}_3}{\partial \hat{y_3}} \cdot \frac{\partial \hat{y_3}}{\partial s_3} \cdot \sum_{k=0}^{3} \frac{\partial s_3}{\partial s_k} \cdot \frac{\partial s_k}{\partial W} \\
    &= \sum_{k=0}^{3} \frac{\partial \mathcal{L}_3}{\partial \hat{y_3}} \cdot \frac{\partial \hat{y_3}}{\partial s_3} \cdot \frac{\partial s_3}{\partial s_k} \cdot \frac{\partial s_k}{\partial W} \\
    &= \sum_{k=0}^{3} \frac{\partial \mathcal{L}_3}{\partial \hat{y_3}} \cdot \frac{\partial \hat{y_3}}{\partial s_3} \cdot \left(\prod_{j=k+1}^{3} \frac{\partial s_j}{\partial s_{j-1}}\right) \cdot \frac{\partial s_k}{\partial W} \\
\end{aligned}
$$
where $\frac{\partial s_3}{\partial s_k}$ itself is a chain rule.</p>
<h2>Vanishing Gradient</h2>
<p>Consider a “simple” RNN with a tanh non-linearity: 
$$
h_t = \tanh(\mathbf{W}h_t + \mathbf{A}x_t)
$$
Explain how vanishing gradients can occur by considering backprop through a hyperbolic tangent function. Would using sigmoid non-linearity instead help this function?</p>
<p>Suppose the loss function is $\mathcal{L}(y_t, \hat{y_t})$, where $\hat{y_t}$ is the output of the network at time $t$. According to backpropagation through time(BPTT), the gradient for $W$ is:</p>
$$
\begin{aligned}
\frac{\partial \mathcal{L}(y_t, \hat{y_t})}{\partial W}
    &= \frac{\partial \mathcal{L}(y_t, \hat{y_t})}{\partial \hat{y_t}} \cdot \frac{\partial \hat{y_t}}{\partial h_t} \cdot \frac{\partial h_t}{\partial W} \\
    &= \sum_{k=0}^{t} \frac{\partial \mathcal{L}(y_t, \hat{y_t})}{\partial \hat{y_t}} \cdot \frac{\partial \hat{y_t}}{\partial h_t} \cdot \frac{\partial h_t}{\partial h_k} \cdot \frac{\partial h_k}{\partial W} \\
    &= \sum_{k=0}^{t} \frac{\partial \mathcal{L}(y_t, \hat{y_t})}{\partial \hat{y_t}} \cdot \frac{\partial \hat{y_t}}{\partial h_t} \cdot \left(\prod_{j=k+1}^{t} \frac{\partial h_j}{\partial h_{j-1}}\right) \cdot \frac{\partial h_k}{\partial W} \\
\end{aligned}
$$<p>To compute the partial gradient of $h_t$, we need to know the gradient of $\tanh$, which is:</p>
$$
\begin{aligned}
\frac{d\tanh(x)}{dx}
    &= \frac{d\frac{\sinh(x)}{\cosh(x)}}{dx}
    = 1 - \tanh^2(x)
    \in [0, 1]
\end{aligned}
$$<p>Then:</p>
$$
\begin{aligned}
\frac{\partial h_j}{\partial h_{j-1}}
    &= \frac{\partial \tanh(\mathbf{W}h_{j-1} + \mathbf{A}x_t)}{\partial h_{j-1}} \\
    &= \left[1 - \tanh^2(\mathbf{W}h_{j-1} + \mathbf{A}x_t) \right] \cdot \mathbf{W} \\
\left(\prod_{j=k+1}^{t} \frac{\partial h_j}{\partial h_{j-1}}\right)
    &= \mathbf{W}^{t-k} \cdot \prod_{j=k+1}^{t} \left[1 - \tanh^2(\mathbf{W}h_{j-1} + \mathbf{A}x_t) \right] \\
    &\approx \mathbf{W}^{t-k} \cdot \epsilon^{t-k} \quad\text{ where } \epsilon \in [0, 1]
\end{aligned}
$$<p>With small values in the matrix $\mathbf{W}$ and large sequence size $t$, the gradient $\frac{\partial \mathcal{L}(y_t, \hat{y_t})}{\partial W}$ would shrink exponentially fast and eventually vanished due to the $(\epsilon^{t-k})$ term.</p>
<p>It is also possible to get exploding gradients due to the $(\mathbf{W}^{t-k})$ term. Nevertheless, this problem is easily detectable during training and can be prevented by setting a maximum value for the gradient in advance.</p>
<p>The gradient of sigmoid is:</p>
$$
\begin{aligned}
\frac{\partial \sigma(x)}{\partial x}
    &= \sigma(x) \cdot [1 - \sigma(x)] \in [0, 1] \\
\end{aligned}
$$<p>The range of this gradient suggests that the sigmoid function would also suffer the vanshing gradient problem as the $\tanh$ function. Usually the $\tanh$ function is preferred because the expected gradient is larger for the same range of input.</p>
<p>Solution to the vanishing gradient problem include using specific activation functions such as ReLU(whose gradient is eight 0 or 1), or using LSTM and GRU architecture.</p>
<p>Reference:</p>
<ul>
<li><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients</a></li>
</ul>

    </div>
    <div id="postamble">
        <p>
            <span class="author">by Jon</span>
            <br>
            <!-- <span class="date">Nov 26, 2018</span> -->
        <p>
    </div>
    <div id="disqus_thread">
      <style type="text/css" scoped>
        @media print {
          #disqus_thread {
            display: none;
          }
        }
      </style>
    </div>
    <script>
    
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://jonioni-github-io.disqus.com/embed.js';
    // s.src = 'https://http-localhost-8000-2iyzu8lfbt.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/tomorrow.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <!-- Languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/ocaml.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/sml.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/tex.min.js"></script>
    <script>document.querySelector(".org-src-container") && document.querySelectorAll("pre.src").forEach(node => { node.classList.add(node.className.match(/src-(.*)/m)[1]); hljs.highlightBlock(node) }) || hljs.initHighlightingOnLoad()</script>
    <script>hljs.initHighlightingOnLoad()</script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH"
        crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js" integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js" integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH"
        crossorigin="anonymous"></script>
    <script>
        renderMathInElement(
            document.body,
            {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false },
                ]
            }
        )
    </script></body>

</html>